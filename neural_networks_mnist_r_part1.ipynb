{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(warn = -1)\n",
    "library(pracma)\n",
    "library(tibble)\n",
    "library(dplyr, warn.conflicts = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning using Neural Networks (Part 1)\n",
    "\n",
    "Welcome to our workshop on Machine Learning using Neural Networks! In this part of the workshop, we will go through some recent applications of neural networks in different areas and cover some background information that you need to know to train your own neural network. We will try to keep the technical details of the neural network algorithm a high-level, so that you can get an understanding of how they work without getting bogged down into tiny details. We will be covering what neural networks are made of, how to train them, and how to evaluate the trained model.\n",
    "\n",
    "We would like to thank Michael Nielson for his excellent online textbook (\"Neural Networks and Deep Learning\", Determination Press, 2015) which we extensively borrowed materials from which can be found [here](http://neuralnetworksanddeeplearning.com).\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Neural Networks\n",
    "\n",
    "Neural Networks represent a class of machine learning algorithms that can learn complex relationships in data. Their popularity has surged recently due to our advances in computational capacity. The algorithms related to neural networks have been around for decades but only in last 15 years or so, we have been able to apply these algorithms to real-world problems.\n",
    "\n",
    "What kind of real-world problems are we solving with neural networks? Here are some examples:\n",
    "\n",
    "- Automatically recognizing people and objects in photographs\n",
    "- Understanding meaning of a text and summarizing it\n",
    "- Recognizing speech and identifying speakers\n",
    "- Making self-driving cars\n",
    "- Identifying diseases form X-rays\n",
    "\n",
    "You migt think that most of these problems are easy to do for regular people, and it is true. Computers are generally good at doing large amounts of calculations very fast, something that people find hard to do. But when we give computers problems to solve that are easy for us - like recognizing a face or words - they perform very poorly at it. It turns out that traditional machine learning algorithms like K-Nearest Neighbors, Support Vector Mahines, Decision Trees etc. are not suitable to do complex tasks that we tend to do easily. This is where Neural Networks come in.\n",
    "\n",
    "Neural Networks have been able to do various complex machine learning tasks with human-like performance. In 2015, researchers at Google trained very deep networks neural networks that <a href=\"http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\">surpassed human performance</a> in image recognition. More advaned Neural Networks can learn to write text and compose music similar to Bach, and they appear in news for playing various games like chess and go better than best human players. More recently, researchers Stanford have also developed a Neural Network model that might be able to <a href=\"https://engineering.stanford.edu/magazine/article/can-algorithm-diagnose-pneumonia-better-radiologist\">diagnose pneumonia better than a radiologist</a>.\n",
    "\n",
    "These achievements of Neural Networks are making them more and more popular.\n",
    "\n",
    "<img src=\"mnist/nn_trends.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How do Neural Networks work?\n",
    "\n",
    "Neural Networks are mostly used for supervised learning problems, where we have input data X and output data Y. To understand more deeply how neural networks use data to make predictions, we need to know a little bit more about what they are made of. Let us go through building blocks of neural networks step-by-step.\n",
    "\n",
    "### Artificial Neurons\n",
    "\n",
    "Neural Networks - as you can guess - are build of artificial neurons. There are many kinds of artificial neurons, and all of them resemble the biological neuron:\n",
    "\n",
    "<img src=\"mnist/bio_neuron.png\"/>\n",
    "\n",
    "Biological neurons receive inputs from multiple dendrites, and based upon the incoming signals and certain chemical properties within them, they decide whether to fire or not. This on and off kind of behavior where either a neuron sends an electrical signal or not was first modeled by an artificial neuron called perceptron.\n",
    "\n",
    "#### Perceptrons\n",
    "\n",
    "In their workings, perceptrons resemble with biological neurons. They have many inputs values and one output value (if there are multiple output arcs, they all carry the same output value). Decisions are applied to input values to decide whether output signal will be sent or not.\n",
    "\n",
    "<img src=\"mnist/percep_1.png\"/>\n",
    "\n",
    "The output is set to 0 or 1 based on some calculation. How do perceptrons decide the output? For each of the inputs, they have a <b>weight</b>. In our example perceptron above, there will be three weights $w_1$, $w_2$, and $w_3$. These weights are multiplied with inputs $x_1$, $x_2$, and $x_3$ to compute three values. These values are then added, and perceptron fires if the sum of these values is above a threhold. More generally:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
    "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
    "      \\end{array} \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "This is how a perceptron works. It uses incoming data to and weights associated with them to decide whether to fire or not. What would happen if we put together many, many perceptrons? Could we build an artificial brain? The answer is not really. It turns out that perceptrons have severe limitations when we use them to build Neural Networks. Their main shortcoming is that they produce very unstable Neural Networks, where a small change in input can cause big change in output. This is linked to the binary behavior of perceptrons. To solve this issue, researchers made a new kind of artifical neurons called sigmoid neurons.\n",
    "\n",
    "#### Sigmoid Neurons\n",
    "\n",
    "Sigmoid neurons differ from perceptrons in one fundamental way: while perceptrons can only give 0 or 1 as their output, sigmoid neurons can also output values between 0 and 1. Sigmoid neurons look exactly like perceptrons:\n",
    "\n",
    "<img src=\"mnist/sigmo_1.png\"/>\n",
    "\n",
    "But they have have a different mathematical formulation. Output of a sigmoid neuron comes by performing two steps. First, we calculate a value $z$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "z = \\sum_j w_j x_j + b\n",
    "\\end{equation}\n",
    "\n",
    "Where $w_j$ are weights associated with each input value, and $b$ is a bias term, similar to the threhold term we use in perceptrons. Then we we pass $z$ through the sigmoid function (which is also called a sqelching function). This operation turns $z$ into a value between 0 and 1:\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\mbox{output} = \\frac{1}{1+e^{-z}}.\n",
    "\\tag{3}\\end{eqnarray}\n",
    "\n",
    "Don't let the formula scare you. You will understand what the squelching function is doing if you look at its graph carefully:\n",
    "\n",
    "<img src=\"mnist/sigmo_2.png\"/>\n",
    "\n",
    "Essentially, for very large positive values of $z$, the sigmoid neuron will output 1. For very large negative values of $z$, the sigmoid neuron will output 0. In other cases, output will be some value between 0 and 1. In a sense, sigmoid neurons allow us to make units of decision making that are more fine grained. For example, imagine if the computer could only tell us whether a given image has a birthday cake or not, versus the computer telling us that there was 87% chance that the picture has a birthday cake. The output of sigmoid neuron can be directly interpreted as a probability that input data have the singnal we are looking for.\n",
    "\n",
    "Let us compute output of a sigmoid neuron:\n",
    "\n",
    "<img src=\"mnist/sigmo_3.png\"/>\n",
    "\n",
    "The weights and bias are as follows:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\begin{array}{l}\n",
    "  w_1 = -2 \\\\\n",
    "  w_2 = -2 \\\\\n",
    "  b = 3      \n",
    "  \\end{array}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Try to compute the outputs of the neurons for following inputs:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\begin{array}{lll}\n",
    "  \\mbox{input 1}: & x_1 = 1 & x_2 = 0.5 \\\\\n",
    "  \\mbox{input 2}: & x_1 = -10 & x_2 = -1\n",
    "  \\end{array}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Execute the code below to find out the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 <- -2 * 1 + -2 * 0.5 + 3\n",
    "output1 <- sigmoid(z1)\n",
    "\n",
    "z2 <- -2 * -10 + -2 * -1 + 3\n",
    "output2 <- sigmoid(z2)\n",
    "\n",
    "cat(\"z1 is\", z1, \"and output1 is\", output1, \"\\n\")\n",
    "cat(\"z2 is\", z2, \"and output2 is\", output2, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! So now you know how fundamental building blocks of neural networks work!\n",
    "\n",
    "Next, we need to know how to put these building blocks together to build neural networks.\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "We can connect many artificial neurons together to build a network of artificial neurons. This is called a neural network. Although, we do not connect the artificial neurons in any arbitrary order. We stack them up in layers, and connect layers with each other. Here is a typical diagram of what a neural network looks like:\n",
    "\n",
    "<img src=\"mnist/arch_1.png\"/>\n",
    "\n",
    "Let us describe all three kinds of layers:\n",
    "\n",
    "- <b>Input layer</b>: This layer consist of (dumb) neurons that simply bring input data into the network. Neurons in this layer are not artificial neurons. Rather, the input neurons just pass input data to the next layer.\n",
    "- <b>Hidden layer</b>: This layer consists of sigmoid neurons that take inputs from input layer neurons, and according to their weights, compute their outputs and pass them to next hidden layer. There can be more than one hidden layers, and in that case, data will keep passing through these hidden layers.\n",
    "- <b>Output layer</b>: This layer is final layer of the neural network, where we read the output from the neuron to make a decision about the input signal that we pass in from the input layer. The output layer can have more than one neurons, in case we want to predict multiple things.\n",
    "\n",
    "If you have heard the term 'deep learning' too often, it simply refers to neural networks that have more than one hidden layer!\n",
    "\n",
    "To use neural networks, we simply pass the input data in input layer. These data propogate through different layers of the network, and we finally get the output values from the output layer. Based on the values from the output layers, we finally make decision about whether the input data contain the relevant signal or not. The signal can be anything from a friend's face in a photograph to utterance of a specific word in speech data.\n",
    "\n",
    "<img src='https://imgs.xkcd.com/comics/machine_learning.png'/>\n",
    "<center>(source- https://xkcd.com/1838/)</center>\n",
    "\n",
    "To do any real tasks with neural networks, we need to train them.\n",
    "\n",
    "### Neural Network Training\n",
    "\n",
    "All supervised learning algorithms require training. During the training procedure, we iteratively go through the data and keep adjusting the weights in the machine learning model to have better and better predictions. A typical training procedure looks like this:\n",
    "\n",
    "- <b>Step 1</b>: Take existing model and generate predictions. These predictions will have some error.\n",
    "- <b>Step 2</b>: Find out where the model is making errors, and what changes should be made to the model weights to reduce the error.\n",
    "- <b>Step 3</b>: Update the model weights ($w_j$ in the case of neural networks).\n",
    "- <b>Step 4</b>: If some threhold is reached (like maximum number of iterations or % accuracy), finish the training procedure. Otherwise go back to Step 1.\n",
    "\n",
    "There is an important caveat here. For the training procedure to work, it is usually required that a small change in weights will only cause only a small change in the output. If this were not the case, the training procedure could go 'haywire'.\n",
    "\n",
    "<img src=\"mnist/train_1.png\"/>\n",
    "\n",
    "All this is good, but you might ask, how much change do we make to weights of the neurons in step 3? Well, the mathematical details are out of the scope of this tutorial, but we can get a general idea by looking at the following equations:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  w_k & \\leftarrow w_k-\\eta \\frac{\\partial C}{\\partial w_k}\\\\\n",
    "  b_l & \\leftarrow b_l-\\eta \\frac{\\partial C}{\\partial b_l}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Essentially, these formulae say that after every iteration through training data, subtract (learning rate $\\eta$) x (gradient vectors) from the weights and biases.\n",
    "\n",
    "Let us understand this in pieces.\n",
    "\n",
    "- Learning rate $\\eta$ decides how fast we want the neural network to learn.\n",
    "- Gradient vectors indicate the amount and direction by which the artificial neuron weights should be updated so that network as a whole makes less error. The amount can be big or small, and the direction can be positive or negative.\n",
    "\n",
    "The weights $w_j$ of artifical neurons are also called parameters of the neural network. The learning rate $\\eta$ is an example of what we call hyperparameters of the neural network. It is not directly related to any artificial neuron, but it still determines the quality of the model. How does that happen? Consider this:\n",
    "\n",
    "<em>If we train the network too fast, the training can again go 'haywire' and we might get a bad model. If we train the network too slow, we might have to wait hours or even days until the model performs in a satisfactory manner.</em>\n",
    "\n",
    "So we need to set $\\eta$ parameter right in order to get a good neural network model. All such parameters of the neural network that are not directly related to the artificial neurons but still affect the model quality are called <b>hyperparameters</b> of the neural network. We need to ensure that during the training, besides finding optimal values of neuron weights, we also set good values for hyperparameters. Finding good values for hyperparameters is considered bit of a black art, because there is no exact procedure to get the best values. The analyst has to proceed in an iterative fashion to find the best values.\n",
    "\n",
    "If all this sounds too spooky to you, let us dive into a really small example! Let us train a single neuron to make correct predictions. We will write a program that goes through 4 steps outlined above.\n",
    "\n",
    "#### Example: Training a Sigmoid Neuron\n",
    "\n",
    "In this example, we will train a single sigmoid neuron. We will start by deciding what the actual weights of the neuron are, and for a random set of data, what the actual predictions should be. After this, we will start our neural network training by assigning random weights to the neuron, and iteratatively update weights until a satisfactory point. We will also be able to tune two hyperparameters: learning rate and maximum number of iterations.\n",
    "\n",
    "Our neuron has three inputs and one output:\n",
    "\n",
    "<img src=\"mnist/sigmo_1.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the REAL weights of neurons which our training algorithm should learn\n",
    "\n",
    "w1 <- -1.0\n",
    "w2 <- 2.0\n",
    "w3 <- 4.0\n",
    "b <- 1.0\n",
    "\n",
    "set.seed(1)\n",
    "\n",
    "# Let us generate some random data points using the real weights, \n",
    "# and make predictions for them\n",
    "\n",
    "df <- tibble(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) %>%\n",
    "  mutate(z = w1 * x1 + w2 * x2 + w3 * x3 + b,\n",
    "         output = if_else(sigmoid(z) > .5, 1, 0)) %>%\n",
    "  select(-z)\n",
    "\n",
    "suppressMessages(attach(df))\n",
    "\n",
    "cat(\"First few rows of training data\")\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we have our training data.\n",
    "\n",
    "# Now let us initialize weights again, but this time randomly.\n",
    "\n",
    "set.seed(1)\n",
    "\n",
    "w1_train <- runif(1, min = -0.5, max = 0.5)\n",
    "w2_train <- runif(1, min = -0.5, max = 0.5)\n",
    "w3_train <- runif(1, min = -0.5, max = 0.5)\n",
    "b_train <- runif(1, min = -0.5, max = 0.5)\n",
    "\n",
    "# Let us also set two hyperparameters\n",
    "\n",
    "learning_rate <- 0.01\n",
    "max_iterations <- 100\n",
    "\n",
    "cat(\"Original neuron weights and biases are w1 =\", w1, \", w2 =\", w2, \", w3 =\", w3, \", b =\", b, \"\\n\\n\")\n",
    "cat(\"Training neuron weights and biases are w1_train =\", w1_train, \", w2_train =\", w2_train,\n",
    "    \", w3_train =\", w3_train, \", b_train =\", b_train, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that training weights are quite different from original weights. The goal of the training algorithm is to learn the original weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron training\n",
    "\n",
    "for (i in 1:max_iterations) {\n",
    "\n",
    "  # Step 1: Take existing model and generate predictions. These predictions will have some error.\n",
    "    \n",
    "  z_train <- w1_train * x1 + w2_train * x2 + w3_train * x3 + b_train\n",
    "  output_train <- sigmoid(z_train)\n",
    "  \n",
    "  \n",
    "  error <- sum((if_else(output_train > .5, 1, 0) - output) != 0)\n",
    "  \n",
    "  cat(\"--- Iteration\", i, \"----\\n\")\n",
    "  cat(\"Weights in step\", i, \"are w1_train =\", w1_train, \", w2_train =\", w2_train,\n",
    "      \", w3_train =\", w3_train, \", b_train =\", b_train, \"\\n\")\n",
    "  cat(\"The neuron is predicting\", error, \"examples incorrectly.\\n\")\n",
    "  \n",
    "  # Step 2: Find out where the model is making error, and what changes should be made to the model weights to reduce the error.\n",
    "  \n",
    "  w1_gradient <- dot(output_train - output, x1)\n",
    "  w2_gradient <- dot(output_train - output, x2)\n",
    "  w3_gradient <- dot(output_train - output, x3)\n",
    "  b_gradient <- sum(output_train - output)\n",
    "  \n",
    "  # Step 3: Update the model weights (w_j in the case of neural networks).\n",
    "  \n",
    "  w1_train <- w1_train - learning_rate * w1_gradient\n",
    "  w2_train <- w2_train - learning_rate * w2_gradient\n",
    "  w3_train <- w3_train - learning_rate * w3_gradient\n",
    "  b_train <- b_train - learning_rate * b_gradient\n",
    "  \n",
    "  # Step 4: If some threhold is reached (like maximum number of iterations or % accuracy),\n",
    "  # finish the training procedure. Otherwise go back to Step 1.\n",
    "    \n",
    "  if (error == 0) {\n",
    "    break\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us again print original and final weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(\"Our algorithm terminated in\", i, \"steps!\", \"\\n\\n\")\n",
    "\n",
    "cat(\"Original neuron weights and biases are w1 =\", w1, \", w2 =\", w2, \", w3 =\", w3, \", b =\", b, \"\\n\\n\")\n",
    "cat(\"Training neuron weights and biases are w1_train =\", w1_train, \", w2_train =\", w2_train,\n",
    "    \", w3_train =\", w3_train, \", b_train =\", b_train, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go back and change learning rate and maximum number of iterations to see how it affects the training procedure.\n",
    "\n",
    "A full neural network training is nothing but changing the weights of many neurons at once. Next examples will not contain all the tiny (and gory) details of training. Rather, we will focus on the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Once we have trained the model, we need to find out how well it performs. Wait! Didn't we already consider model accuracy when we were training it? If during the training, we were trying to change the artificial neuron weights to make the model more accurate, didn't we end up with a measure of accuracy at the end of the procedure? You are right, we did. But we cannot use that error rate as the error rate of the model. Why?\n",
    "\n",
    "The training procedure aims to optimize (or change) the parameters of the model such that model can make predictions as accurate as possible. But in this process, our model ends up 'seeing' the data we use to train it, many many times. So if we tried to make model predictions for data we used to train it, we would be 'cheating' in a sense. The data used to train the model are called <b>training data</b>. If we use training data to evaluate our model, we would get an overly optimistic estimate of model accuracy.\n",
    "\n",
    "To avoid this trap, data scientists typically keep a hold out set of data called <b>test data</b>. Test set data are never seen during the model training procedure. Only when the final model is trained and ready, we pass the test set through it once, and find out how much error the model makes. This can give us a more honest estimate of model accuracy.\n",
    "\n",
    "<b>Never estimate the model accuracy using training data!</b>\n",
    "\n",
    "Here is a small diagram, depicting the correct way to estimate model accuracy as described above:\n",
    "\n",
    "<img src=\"mnist/eval_1.png\"/>\n",
    "\n",
    "So far, we have not talked about what are we trying to predict with the neural network. The two most popular types of predictions people make with neural network are class labels and numeric values. Predicting class labels is referred to as classification, and it involves predicting the correct label for the input data. There can be two or more classes. For example, predicting whether there is a moon in the picture is a <b>binary classification</b> problem, while predicting what handwritten digit is there in the given picture is a <b>multi-class classification</b> problem. Predicting numeric values is referred to as <b>regression</b>, and involves predicinng numeric values like house prices, stock prices etc.\n",
    "\n",
    "Let us see what simple accuracy metrics are available for different classification and regression problems.\n",
    "\n",
    "#### Binary Classification\n",
    "\n",
    "Binary classification refers to a problem where given the input data, we want to <b>predict yes or no</b> (or in general, whether the input data contain the signal of interest or not). There are way too many metrics to measure accuracy of binary classification models. Some of the important ones are are accuracy, balanced accuracy, sensitivity, specificity and Cohen's Kappa.\n",
    "\n",
    "- <b>Accuracy</b>: % of times the model predicted the outcome correctly\n",
    "- <b>Balanced misclassification rate</b>: Average of % of times the model predicted yes correctly and % of times the model predicted no correctly\n",
    "- <b>Sensitivity</b>: % of times the model predicts yes, given that the actual label is yes\n",
    "- <b>Specificity</b>: % of times the model predicts no, given that the actual label is no\n",
    "- <b>Cohen's Kappa</b>: % improvement over <b>baseline accuracy</b>\n",
    "\n",
    "So what is baseline accuracy? Baseline accuracy is accuracy that we can achieve without any model training. Consider training data where 90% of the data points have yes as the outcome value. If our model simply predicted yes, we would have a model with 90% accuracy right off the bat! In this case, 90% is the baseline accuracy, and since there is no improvement over the baseline accuracy, Cohen's kappa would be 0. In case the model accuracy was 95%, Cohen's Kappa would be 0.5, indicating that we have covered 50% of the distance between baseline accuracy and 100% accuracy.\n",
    "\n",
    "It is also common to draw up something called the confusion matrix for binary classifiers.\n",
    "\n",
    "<img src=\"mnist/eval_2.png\"/>\n",
    "\n",
    "- TP: True positive\n",
    "- TN: True negative\n",
    "- FP: False positive\n",
    "- FN: False negative\n",
    "\n",
    "With the help of this matrix, we can calcualte the terms we described earlier. For example:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{Accuracy} = \\frac{TP + TN}{TP + FP + TN + FN} \\\\\n",
    "  \\mbox{Balanced Accuracy} = \\frac{\\frac{TP}{TP + TN} + \\frac{FP}{FP + FN}}{2}\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Multi-Class Classification\n",
    "\n",
    "For multi-class classification, we cannot directly use all the accuracy metrics from binary classification. Although it is very common to use misclassification rate to estimate the accuracy of a multi-class classifier. Balanced accuracy can also be used.\n",
    "\n",
    "#### Regression\n",
    "\n",
    "Regression involes predicting numeric values. So a typical way to measure accuracy of a regression model is to calculate how far predictions are from the actual outputs on an average. A typical measure for this is <b>Root Mean Squared Error</b>, which is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mbox{RMSE} = \\sqrt{\\frac{\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2}{n}}\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\hat{y_i}$ refers to prediction made by the model, $y_i$ refers to the actual value that the model should have predicted, and $n$ refers to the number of data points.\n",
    "\n",
    "A model with low RMSE is preferred. There are other metrics for assessing accuracy of a regression model, but we will not cover them here.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Let us recap. We started by understanding basic building blocks of neural networks - artificial neurons. We saw how these neurons are put together in layers to build neural networks. We briefly outlined how the neural networks are trained to make useful predictions, and how to evaluate the model quality after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Please continue in the next notebook page by clicking here](neural_networks_mnist_r_part2.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
