{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(warn = -1)\n",
    "library(pracma)\n",
    "library(tibble)\n",
    "library(dplyr, warn.conflicts = FALSE)\n",
    "library(ggplot2)\n",
    "library(tidyr)\n",
    "library(stringr)\n",
    "library(ggthemes)\n",
    "library(mxnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(\"mnist/mnist_data.RData\")\n",
    "mnist_data_mat <- data.matrix(mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(mnist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning using Neural Networks (Part 2)\n",
    "\n",
    "Welcome to the tutorial on Machine Learning using Neural Networks! In this part of the tutorial, we will use neural networks to recognize handwritten digits. Will will start by outlining why neural networks are good for recognizing images, and then train a neural network to recognize handwritten digits. We will use a famous dataset for this task, which is called MNIST.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Recognition using Neural Networks\n",
    "\n",
    "How can computer programs recognize objects in a photograph? This has been one of the central questions in the computer vision research. It is very simple for humans to recognize objects in the photographs, just like we do in real world. But for computers, this task becomes notoriously difficult! Although with a little help from neural networks, it becomes rather easy because neural networks behave in a slightly intuitive way. Here is how a neural network would recognize whether there is a face in an image or not:\n",
    "\n",
    "<img src=\"mnist/imgrecog_1.png\"/>\n",
    "\n",
    "What you see in the image above essentially tell us that neural networks try to recognize different 'features' of the image first, and based on appearance of those features, they decide whether the image contains the signal of interest or not (a human face in this case).\n",
    "\n",
    "The problem of detecting a face in a photograph is a binary classification problem, but for our example, we are going to turn to a multi-class classification problem.\n",
    "\n",
    "## Example: Recognizing Handwritten Digits using Neural Networks\n",
    "\n",
    "Okay! We're in the final section of our workshop. In this section, we will try train a neural network model that can tell us which handwritten digit is there in the image we give to it. We are going to use a database of images that was originally developed in 1994 called the MNIST dataset. It is based on samples of high school students and U. S. Census Bureau employees' writing- https://en.wikipedia.org/wiki/MNIST_database\n",
    "\n",
    "<img src=\"mnist/mnist_1.png\"/>\n",
    "\n",
    "You can see that there are many different ways of wriring 0s, 1s, 2s, and so on. How is neural network going to detect which digit is there in the image? It turns out that neural network will try to break down the problem in parts and work it out.\n",
    "\n",
    "### Detecting a 0\n",
    "\n",
    "Let us say the neural network is trying to detect whether there is 0 in the image or not. It will proceed like this:\n",
    "\n",
    "- Detect handwriting strokes in different parts of the image\n",
    "- Put these strokes together to figure out what digit is there in the image\n",
    "\n",
    "For a 0, we might have these strokes in the image:\n",
    "\n",
    "<img src=\"mnist/mnist_2.png\" width=350/>\n",
    "\n",
    "where the final image looks like this:\n",
    "\n",
    "<img src=\"mnist/mnist_3.png\" width=105/>\n",
    "\n",
    "And remember, there can be many different types of 0s! But most of them will have circular strokes in all four sides of the image.\n",
    "\n",
    "The code below plots images that were labled (the training data) as zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_pos <- which(mnist_data$label == 0)\n",
    "\n",
    "options(repr.plot.width = 8, repr.plot.height = 10)\n",
    "\n",
    "mnist_data %>%\n",
    "  slice(zero_pos[1:100]) %>%\n",
    "  mutate(example = row_number()) %>%\n",
    "  gather(key = pixnum, value = inten, -example, -label) %>%\n",
    "  mutate(pixnum = as.integer(str_replace(pixnum, \"pixel\", \"\")) + 1,\n",
    "         x = factor(floor(pixnum / 28) + 1, 28:1),\n",
    "         y = factor(pixnum %% 28 + 1, 1:28)) %>%\n",
    "  drop_na() %>%\n",
    "  ggplot(aes(y, x)) +\n",
    "  geom_tile(aes(fill = inten)) +\n",
    "  facet_wrap(~ example, nrow = 10) +\n",
    "  scale_fill_continuous(low = \"white\", high = \"black\") +\n",
    "  theme_minimal() +\n",
    "  theme(legend.position = \"none\",\n",
    "        axis.ticks = element_blank(),\n",
    "        axis.text = element_blank(),\n",
    "        axis.title = element_blank()) +\n",
    "  ggtitle(\"100 different handwritten 0s from MNIST dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting 0 to 9\n",
    "\n",
    "So you now have a rough idea about how can a neural network detect 0 in an image. However, we need to detect all the digits, from 0 to 9. How will we do this using using a neural network?\n",
    "\n",
    "So far, we have only talked about binary outcomes where a neural network is detecting whether there is 'some thing' present in its input or not. This requires us to have one neuron in the output layer. But if we want to detect multiple things, we need to have multiple neurons in the output layer, one for each thing we need to detect. In our case, we need to detect all the arabic numerals, so there will be 10 output nodes. Here is what our neural network will look like:\n",
    "\n",
    "<img src=\"mnist/mnist_4.png\"/>\n",
    "\n",
    "Let us find out where those 784 input neurons are coming from.\n",
    "\n",
    "### MNIST Dataset\n",
    "\n",
    "The MNIST dataset contains many thousand training examples for handwritten digits. Each training example is a 28x28 pixel image. This means that we have 28x28 = 784 features for each of our training example. As the output, we have a numeric value from 0 to 9, which tells the actual digit that is stored in those 784 pixels. We are going to use this information to train and evalulate our model.\n",
    "\n",
    "Before we jump further in, you might ask, what do these 784 pixels contain? They contain what is called pixel intensity value, a value between 0 and 255 which indicates how dark the pixel is. If we print these pixel intensity values, we get to see the actual image. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 7, repr.plot.height = 6)\n",
    "\n",
    "mnist_data %>%\n",
    "  slice(66) %>%\n",
    "  mutate(example = row_number()) %>%\n",
    "  gather(key = pixnum, value = inten, -example, -label) %>%\n",
    "  mutate(pixnum = as.integer(str_replace(pixnum, \"pixel\", \"\")) + 1,\n",
    "         x = factor(floor(pixnum / 28) + 1, 28:1),\n",
    "         y = factor(pixnum %% 28 + 1, 1:28)) %>%\n",
    "  drop_na() %>%\n",
    "  ggplot(aes(y, x)) +\n",
    "  geom_tile(aes(fill = inten, width = .8, height = .8), size = 1, alpha = .6) +\n",
    "  geom_text(aes(label = inten), size = 1.8, color = \"black\") +\n",
    "  scale_fill_continuous(low = \"white\", high = \"black\") +\n",
    "  theme(axis.ticks = element_blank(),\n",
    "        axis.text = element_blank(),\n",
    "        axis.title = element_blank()) +\n",
    "  labs(fill = \"Intensity value\", title = \"A digit 1 with pixel intensities from MNIST dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to feed a lot of these images to our neural network, and it will learn characteristics of different digits for us. Then we can use this trained model to make predictions.\n",
    "\n",
    "### Model Training\n",
    "\n",
    "Let us start the model training by looking at the the dataset. We will use <b>mxnet</b> (https://github.com/Xilinx/mxnet/tree/master/R-package) package for our machine learning task. Here are some rows from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(mnist_data_mat, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will divide the data in <b>training set</b> and <b>test set</b>. Our training set will have 35,000 examples and our test set will have 7,000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test <- mnist_data_mat[1:7000,]\n",
    "train <- mnist_data_mat[7001:42000,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will organize the data into objects representing the x and y variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.x <- train[,-1]\n",
    "train.y <- train[,1]\n",
    "train.x <- t(train.x/255)\n",
    "\n",
    "test.x <- test[,-1]\n",
    "test.y <- test[,1]\n",
    "test.x <- t(test.x/255)\n",
    "\n",
    "cat(\"Training data dimensions\")\n",
    "dim(train.x)\n",
    "table(train.y)\n",
    "\n",
    "cat(\"Test data dimensions\")\n",
    "dim(test.x)\n",
    "table(test.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will configure our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "data <- mx.symbol.Variable(\"data\")\n",
    "\n",
    "# Hidden layer with 15 neurons\n",
    "fc1 <- mx.symbol.FullyConnected(data, name=\"fc1\", num_hidden=15)\n",
    "act1 <- mx.symbol.Activation(fc1, name=\"tanh1\", act_type=\"tanh\")\n",
    "\n",
    "# Output layer\n",
    "fc2 <- mx.symbol.FullyConnected(act1, name=\"fc2\", num_hidden=10)\n",
    "softmax <- mx.symbol.SoftmaxOutput(fc2, name=\"sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set the hyperparameters of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate <- 0.02\n",
    "max_iterations <- 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices <- mx.cpu()\n",
    "mx.set.seed(0)\n",
    "\n",
    "model <- mx.model.FeedForward.create(softmax, X=train.x, y=train.y,\n",
    "                                     ctx=devices, num.round=max_iterations, array.batch.size=100,\n",
    "                                     learning.rate=learning_rate, momentum=0.9,  eval.metric=mx.metric.accuracy,\n",
    "                                     initializer=mx.init.uniform(0.07),\n",
    "                                     epoch.end.callback=mx.callback.log.train.metric(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that our model is trained, we will generate some predictions and assess the accuracy. <b>Remember, always assess accuracy using test data!</b>\n",
    "\n",
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds <- predict(model, test.x)\n",
    "dim(preds) # this is the shape of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the distribution of the answers in a comparison matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.label <- max.col(t(preds)) - 1\n",
    "table(pred = pred.label, orig = test.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the missed predictions.  Do you see a pattern?\n",
    "\n",
    "Let us calculate the test accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_accuracy <- mean(pred.label == test.y)\n",
    "cat(\"Test accuracy of the model is\", pct_accuracy * 100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. How can we improve this model? There are different ways to improve predictions of a neural network:\n",
    "\n",
    "- <b>Change architecture</b>\n",
    "- <b>Try different hyperparameters</b>\n",
    "- <b>Try advanced methods like convolution</b>\n",
    "\n",
    "Trying these different things is out of the scope of this tutorial, but if you want to explore these options, you can do so in <a href=\"https://playground.tensorflow.org\">TensorFlow Playground</a>.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this part of the tutorial, we saw how neural networks are trained using software packages. Don't worry if this was too complicated, you always get help about doing complex things when working in teams. Websites like [StackOverflow](https://stackoverflow.com/) can also help in case you get stuck.\n",
    "\n",
    "We hope that you found both part 1 and part 2 of the tutorial useful.\n",
    "\n",
    "## References\n",
    "\n",
    "We would like to thank Michael Nielson for his excellent online textbook on neural networks. We extensively borrowed part 1 materials from his textbook which can be found <a href=\"http://neuralnetworksanddeeplearning.com\">here</a>.\n",
    "\n",
    "For part 2, we used the mxnet example presented <a href=\"https://mxnet.incubator.apache.org/tutorials/r/mnistCompetition.html\">here</a>, but modified it according to our neural network architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
